{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a8f9622945156d6337ba73c481da2de7efef7384"
   },
   "source": [
    "## <div style=\"text-align: center\">Data Cleaning in Python for Machine Learning Experts</div>\n",
    "\n",
    "<div style=\"text-align: center\">There are plenty of <b>courses and tutorials</b> that can help you learn machine learning from scratch but here in <b>Kaggle</b>, I want to predict <b>House prices</b>(in the next version)  a popular machine learning Dataset as a comprehensive workflow with python packages. \n",
    "After reading, you can use this workflow to solve other real problems and use it as a template to deal with <b>machine learning</b> problems.</div>\n",
    "<div style=\"text-align:center\">last update: <b>10/15/2018</b></div>\n",
    "\n",
    "\n",
    "\n",
    ">###### you may  be interested have a look at it: [**A Comprehensive ML Workflow for House Prices**](https://www.kaggle.com/mjbahmani/a-comprehensive-ml-workflow-for-house-prices)\n",
    "\n",
    "\n",
    "---------------------------------------------------------------------\n",
    "you can follow me on:\n",
    "> ###### [ GitHub](https://github.com/mjbahmani)\n",
    "> ###### [Kaggle](https://www.kaggle.com/mjbahmani/)\n",
    "-------------------------------------------------------------------------------------------------------------\n",
    " **I hope you find this kernel helpful and some upvotes would be very much appreciated**\n",
    " \n",
    " -----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cda11210a88d6484112cbe2c3624225328326c6a"
   },
   "source": [
    "## Notebook  Content\n",
    "*   1-  [Introduction](#1)\n",
    "*   2- [Machine Learning workflow](#2)\n",
    "*   3- [Problem Definition](#3)\n",
    "*       3-1 [Problem feature](#4)\n",
    "*       3-2 [Aim](#5)\n",
    "*       3-3 [Variables](#6)\n",
    "*   4-[ Inputs & Outputs](#7)\n",
    "*   4-1 [Inputs ](#8)\n",
    "*   4-2 [Outputs](#9)\n",
    "*   5- [Installation](#10)\n",
    "*       5-1 [ jupyter notebook](#11)\n",
    "*       5-2[ kaggle kernel](#12)\n",
    "*       5-3 [Colab notebook](#13)\n",
    "*       5-4 [install python & packages](#14)\n",
    "*       5-5 [Loading Packages](#15)\n",
    "*   6- [Exploratory data analysis](#16)\n",
    "*       6-1 [Data Collection](#17)\n",
    "*       6-2 [Visualization](#18)\n",
    "*           6-2-1 [Scatter plot](#19)\n",
    "*           6-2-2 [Box](#20)\n",
    "*           6-2-3 [Histogram](#21)\n",
    "*           6-2-4 [Multivariate Plots](#22)\n",
    "*           6-2-5 [Violinplots](#23)\n",
    "*           6-2-6 [Pair plot](#24)\n",
    "*           6-2-7 [Kde plot](#25)\n",
    "*           6-2-8 [Joint plot](#26)\n",
    "*           6-2-9 [Andrews curves](#27)\n",
    "*           6-2-10 [Heatmap](#28)\n",
    "*           6-2-11 [Radviz](#29)\n",
    "*       6-3 [Data Preprocessing](#30)\n",
    "*       6-4 [Data Cleaning](#31)\n",
    "*   7- [Model Deployment](#32)\n",
    "*       7-1[ KNN](#33)\n",
    "*       7-2 [Radius Neighbors Classifier](#34)\n",
    "*       7-3 [Logistic Regression](#35)\n",
    "*       7-4 [Passive Aggressive Classifier](#36)\n",
    "*       7-5 [Naive Bayes](#37)\n",
    "*       7-6 [MultinomialNB](#38)\n",
    "*       7-7 [BernoulliNB](#39)\n",
    "*       7-8 [SVM](#40)\n",
    "*       7-9 [Nu-Support Vector Classification](#41)\n",
    "*       7-10 [Linear Support Vector Classification](#42)\n",
    "*       7-11 [Decision Tree](#43)\n",
    "*       7-12 [ExtraTreeClassifier](#44)\n",
    "*       7-13 [Neural network](#45)\n",
    "*            7-13-1 [What is a Perceptron?](#45)\n",
    "*       7-14 [RandomForest](#46)\n",
    "*       7-15 [Bagging classifier ](#47)\n",
    "*       7-16 [AdaBoost classifier](#48)\n",
    "*       7-17 [Gradient Boosting Classifier](#49)\n",
    "*       7-18 [Linear Discriminant Analysis](#50)\n",
    "*       7-19 [Quadratic Discriminant Analysis](#51)\n",
    "*       7-20 [Kmeans](#52)\n",
    "*       7-21 [Backpropagation](#53)\n",
    "*   9- [Conclusion](#54)\n",
    "*  10- [References](#55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "750903cc2679d39058f56df6c6c040be02b748df"
   },
   "source": [
    " <a id=\"1\"></a> <br>\n",
    "## 1- Introduction\n",
    "This is a **comprehensive ML techniques with python** , that I have spent for more than two months to complete it.\n",
    "\n",
    "it is clear that everyone in this community is familiar with IRIS dataset but if you need to review your information about the dataset please visit this [link](https://archive.ics.uci.edu/ml/datasets/iris).\n",
    "\n",
    "I have tried to help **beginners**  in Kaggle how to face machine learning problems. and I think it is a great opportunity for who want to learn machine learning workflow with python completely.\n",
    "I have covered most of the methods that are implemented for iris until **2018**, you can start to learn and review your knowledge about ML with a simple dataset and try to learn and memorize the workflow for your journey in Data science world.\n",
    "\n",
    "I am open to getting your feedback for improving this **kernel**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e11b73b618b0f6e4335520ef80267c6d577d1ba5"
   },
   "source": [
    "<a id=\"2\"></a> <br>\n",
    "## 2- Machine Learning Workflow\n",
    "Field of \tstudy \tthat \tgives\tcomputers\tthe\tability \tto\tlearn \twithout \tbeing\n",
    "explicitly \tprogrammed.\n",
    "\n",
    "Arthur\tSamuel, 1959\n",
    "\n",
    "If you have already read some [machine learning books](https://towardsdatascience.com/list-of-free-must-read-machine-learning-books-89576749d2ff). You have noticed that there are different ways to stream data into machine learning.\n",
    "\n",
    "most of these books share the following steps (checklist):\n",
    "*   Define the Problem(Look at the big picture)\n",
    "*   Specify Inputs & Outputs\n",
    "*   Data Collection\n",
    "*   Exploratory data analysis\n",
    "*   Data Preprocessing\n",
    "*   Model Design, Training, and Offline Evaluation\n",
    "*   Model Deployment, Online Evaluation, and Monitoring\n",
    "*   Model Maintenance, Diagnosis, and Retraining\n",
    "\n",
    "**You can see my workflow in the below image** :\n",
    " <img src=\"http://s9.picofile.com/file/8338227634/workflow.png\" />\n",
    "\n",
    "**you should\tfeel free\tto\tadapt \tthis\tchecklist \tto\tyour needs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "89ee0cda57822cd4102eadf8992c5bfe1964d557"
   },
   "source": [
    "<a id=\"10\"></a> <br>\n",
    "## 5-Installation\n",
    "#### Windows:\n",
    "* Anaconda (from https://www.continuum.io) is a free Python distribution for SciPy stack. It is also available for Linux and Mac.\n",
    "* Canopy (https://www.enthought.com/products/canopy/) is available as free as well as commercial distribution with full SciPy stack for Windows, Linux and Mac.\n",
    "* Python (x,y) is a free Python distribution with SciPy stack and Spyder IDE for Windows OS. (Downloadable from http://python-xy.github.io/)\n",
    "#### Linux\n",
    "Package managers of respective Linux distributions are used to install one or more packages in SciPy stack.\n",
    "\n",
    "For Ubuntu Users:\n",
    "sudo apt-get install python-numpy python-scipy python-matplotlibipythonipythonnotebook\n",
    "python-pandas python-sympy python-nose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c1793fb141d3338bbc4300874be6ffa5cb1a9139"
   },
   "source": [
    "<a id=\"11\"></a> <br>\n",
    "## 5-1 Jupyter notebook\n",
    "I strongly recommend installing **Python** and **Jupyter** using the **[Anaconda Distribution](https://www.anaconda.com/download/)**, which includes Python, the Jupyter Notebook, and other commonly used packages for scientific computing and data science.\n",
    "\n",
    "First, download Anaconda. We recommend downloading Anaconda’s latest Python 3 version.\n",
    "\n",
    "Second, install the version of Anaconda which you downloaded, following the instructions on the download page.\n",
    "\n",
    "Congratulations, you have installed Jupyter Notebook! To run the notebook, run the following command at the Terminal (Mac/Linux) or Command Prompt (Windows):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "abbd1757dde9805758a2cec47a186e31dbc29822"
   },
   "source": [
    "> jupyter notebook\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8a70c253d5afa93f07a7a7e048dbb2d7812c8d10"
   },
   "source": [
    "<a id=\"12\"></a> <br>\n",
    "## 5-2 Kaggle Kernel\n",
    "Kaggle kernel is an environment just like you use jupyter notebook, it's an **extension** of the where in you are able to carry out all the functions of jupyter notebooks plus it has some added tools like forking et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "237bbe4e4509c9491ce165e3599c432b979d7b90"
   },
   "source": [
    "<a id=\"13\"></a> <br>\n",
    "## 5-3 Colab notebook\n",
    "**Colaboratory** is a research tool for machine learning education and research. It’s a Jupyter notebook environment that requires no setup to use.\n",
    "### 5-3-1 What browsers are supported?\n",
    "Colaboratory works with most major browsers, and is most thoroughly tested with desktop versions of Chrome and Firefox.\n",
    "### 5-3-2 Is it free to use?\n",
    "Yes. Colaboratory is a research project that is free to use.\n",
    "### 5-3-3 What is the difference between Jupyter and Colaboratory?\n",
    "Jupyter is the open source project on which Colaboratory is based. Colaboratory allows you to use and share Jupyter notebooks with others without having to download, install, or run anything on your own computer other than a browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fbedcae8843986c2139f18dad4b5f313e6535ac5"
   },
   "source": [
    "<a id=\"15\"></a> <br>\n",
    "## 5-5 Loading Packages\n",
    "In this kernel we are using the following packages:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "61f49281fdd8592b44c0867225f57e6fce36342c"
   },
   "source": [
    " <img src=\"http://s8.picofile.com/file/8338227868/packages.png\">\n",
    " Now we import all of them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.6.1 |Anaconda 4.4.0 (64-bit)| (default, May 11 2017, 13:25:24) [MSC v.1900 64 bit (AMD64)]\n",
      "scipy: 0.19.0\n",
      "matplotlib: 2.0.2\n",
      "numpy: 1.12.1\n",
      "pandas: 0.20.1\n",
      "seaborn: 0.7.1\n",
      "matplotlib: 2.0.2\n",
      "sklearn: 0.18.1\n"
     ]
    }
   ],
   "source": [
    "# packages to load \n",
    "# Check the versions of libraries\n",
    "# Python version\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys\n",
    "print('Python: {}'.format(sys.version))\n",
    "# scipy\n",
    "import scipy\n",
    "print('scipy: {}'.format(scipy.__version__))\n",
    "import numpy\n",
    "# matplotlib\n",
    "import matplotlib\n",
    "print('matplotlib: {}'.format(matplotlib.__version__))\n",
    "# numpy\n",
    "import numpy as np # linear algebra\n",
    "print('numpy: {}'.format(np.__version__))\n",
    "# pandas\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "print('pandas: {}'.format(pd.__version__))\n",
    "import seaborn as sns\n",
    "print('seaborn: {}'.format(sns.__version__))\n",
    "sns.set(color_codes=True)\n",
    "import matplotlib.pyplot as plt\n",
    "print('matplotlib: {}'.format(matplotlib.__version__))\n",
    "%matplotlib inline\n",
    "# scikit-learn\n",
    "import sklearn\n",
    "print('sklearn: {}'.format(sklearn.__version__))\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "import os\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Importing metrics for evaluation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "04ff1a533119d589baee777c21194a951168b0c7"
   },
   "source": [
    "<a id=\"16\"></a> <br>\n",
    "## 6- Data Cleaning\n",
    " In this section, you'll learn how to use graphical and numerical techniques to begin uncovering the structure of your data. \n",
    " \n",
    "* Which variables suggest interesting relationships?\n",
    "* Which observations are unusual?\n",
    "\n",
    "By the end of the section, you'll be able to answer these questions and more, while generating graphics that are both insightful and beautiful.  then We will review analytical and statistical operations:\n",
    "\n",
    "*   5-1 Data Collection\n",
    "*   5-2 Visualization\n",
    "*   5-3 Data Preprocessing\n",
    "*   5-4 Data Cleaning\n",
    "<img src=\"http://s9.picofile.com/file/8338476134/EDA.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cedecea930b278f86292367cc28d2996a235a169"
   },
   "source": [
    "<a id=\"17\"></a> <br>\n",
    "## 6-1 Data Collection\n",
    "**Data collection** is the process of gathering and measuring data, information or any variables of interest in a standardized and established manner that enables the collector to answer or test hypothesis and evaluate outcomes of the particular collection.[techopedia]\n",
    "\n",
    "**Iris dataset**  consists of 3 different types of irises’ (Setosa, Versicolour, and Virginica) petal and sepal length, stored in a 150x4 numpy.ndarray\n",
    "\n",
    "The rows being the samples and the columns being: Sepal Length, Sepal Width, Petal Length and Petal Width.[6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "9269ae851b744856bce56840637030a16a5877e1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import Dataset to play with it\n",
    "dataset = pd.read_csv('../input/Iris.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "58ed9c838069f54de5cf90b20a774c3e236149b3"
   },
   "source": [
    "**<< Note 1 >>**\n",
    "\n",
    "* Each row is an observation (also known as : sample, example, instance, record)\n",
    "* Each column is a feature (also known as: Predictor, attribute, Independent Variable, input, regressor, Covariate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7b5fd1034cd591ebd29fba1c77d342ec2b408d13"
   },
   "source": [
    "After loading the data via **pandas**, we should checkout what the content is, description and via the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "edd043f8feb76cfe51b79785302ca4936ceb7b51"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5af51158a5bc342947c553392e3d1665ac24ba62"
   },
   "source": [
    "### 6-2-12 Conclusion\n",
    "we have used Python to apply data visualization tools to the Iris dataset. Color and size changes were made to the data points in scatterplots. I changed the border and fill color of the boxplot and violin, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "581b90e6a869c3793472c7edd59091d6d6342fb2"
   },
   "source": [
    "## 6-3-1 Features\n",
    "Features:\n",
    "* numeric\n",
    "* categorical\n",
    "* ordinal\n",
    "* datetime\n",
    "* coordinates\n",
    "\n",
    "find the type of features in titanic dataset\n",
    "<img src=\"http://s9.picofile.com/file/8339959442/titanic.png\" height=\"700\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-3-2 Explorer Dataset\n",
    "1- Dimensions of the dataset.\n",
    "\n",
    "2- Peek at the data itself.\n",
    "\n",
    "3- Statistical summary of all attributes.\n",
    "\n",
    "4- Breakdown of the data by the class variable.[7]\n",
    "\n",
    "Don’t worry, each look at the data is **one command**. These are useful commands that you can use again and again on future projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "4b45251be7be77333051fe738639104ae1005fa5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 6)\n"
     ]
    }
   ],
   "source": [
    "# shape\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c64e9d3e0bf394fb833de94a0fc5c34f69fce24c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#columns*rows\n",
    "dataset.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "277e1998627d6a3ddeff4e913a6b8c3dc81dec96"
   },
   "source": [
    "\n",
    "We can get a quick idea of how many instances (rows) and how many attributes (columns) the data contains with the shape property.\n",
    "\n",
    "You should see 150 instances and 5 attributes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "95ee5e18f97bc410df1e54ac74e32cdff2b30755"
   },
   "source": [
    "for getting some information about the dataset you can use **info()** command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ca840f02925751186f87e402fcb5f637ab1ab8a0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(dataset.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3458838205be4c7fbff88e95ef69934e13e2199b"
   },
   "source": [
    "you see number of unique item for Species with command below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4b90d165a007106ae99809ad28edd75bd8153dd8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset['Species'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8accfbddf2228274ad412c3ad3be72b4107d6f6c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset[\"Species\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ae08b544a8d4202c7d0a47ec83d685e81c91a66d"
   },
   "source": [
    "to check the first 5 rows of the data set, we can use head(5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5899889553c3416b27e93efceddb106eb71f5156",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.head(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1150b6ac3d82562aefd5c64f9f01accee5eace4d"
   },
   "source": [
    "to check out last 5 row of the data set, we use tail() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "79339442ff1f53ae1054d794337b9541295d3305",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.tail() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2c288c3dc8656a872a8529368812546e434d3a22"
   },
   "source": [
    "to pop up 5 random rows from the data set, we can use **sample(5)**  function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "09eb18d1fcf4a2b73ba2f5ddce99dfa521681140",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.sample(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c8a1cc36348c68fb98d6cb28aa9919fc5f2892f3"
   },
   "source": [
    "to give a statistical summary about the dataset, we can use **describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3f7211e96627b9a81c5b620a9ba61446f7719ea3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.describe() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "031d16ca235837e889734635ecff193be64b27a4"
   },
   "source": [
    "to check out how many null info are on the dataset, we can use **isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8807b632269e2fa734ad26e8513199400fc09a83",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "446e6162e16325213047ff31454813455668b574",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.groupby('Species').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c2f1eaf0b6dfdc7cc4dace04614e99ed56425d00"
   },
   "source": [
    "to print dataset **columns**, we can use columns atribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "909d61b33ec06249d0842e6115597bbacf21163f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "22bc5d81c18275ee1fb082c0adbb7a65bdbec4cc"
   },
   "source": [
    "**<< Note 2 >>**\n",
    "in pandas's data frame you can perform some query such as \"where\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c8c8d9fd63d9bdb601183aeb4f1435affeb8a596",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.where(dataset ['Species']=='Iris-setosa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "33fc33a18489b438a884819d99dc00a02b113be8"
   },
   "source": [
    "as you can see in the below in python, it is so easy perform some query on the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8b545ff7e8367c5ab9c1db710f70b6936ac8422c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset[dataset['SepalLengthCm']>7.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1c92b300076a232321c915857d8a7c5685a97865",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Seperating the data into dependent and independent variables\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<< Note >>**\n",
    ">**Preprocessing and generation pipelines depend on a model type**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8280749a19af32869978c61941d1dea306632d71"
   },
   "source": [
    "<a id=\"31\"></a> <br>\n",
    "## 6-4 Data Cleaning\n",
    "When dealing with real-world data, dirty data is the norm rather than the exception. We continuously need to predict correct values, impute missing ones, and find links between various data artefacts such as schemas and records. We need to stop treating data cleaning as a piecemeal exercise (resolving different types of errors in isolation), and instead leverage all signals and resources (such as constraints, available statistics, and dictionaries) to accurately predict corrective actions.\n",
    "\n",
    "The primary goal of data cleaning is to detect and remove errors and **anomalies** to increase the value of data in analytics and decision making. While it has been the focus of many researchers for several years, individual problems have been addressed separately. These include missing value imputation, outliers detection, transformations, integrity constraints violations detection and repair, consistent query answering, deduplication, and many other related problems such as profiling and constraints mining.[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e8e4da1a0b3d51a5fff38750fb4631ac3aa7eebb"
   },
   "source": [
    "This is an example that I have taken from a draft of the 3rd edition of Jurafsky and Martin, with slight modifications:\n",
    "We import *numpy* and use its *exp* function. We could use the same function from the *math* module, or some other module like *scipy*. The *sigmoid* function is defined as in the textbook:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "97adc471c068fbd8d36ca19a4db0d98b0924c731"
   },
   "source": [
    "-----------------\n",
    "<a id=\"54\"></a> <br>\n",
    "# 8- Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1adfb5ba84e0f1d8fba58a2fca30546ead095047",
    "collapsed": true
   },
   "source": [
    "In this kernel, I have tried to cover all the parts related to the process of ML with a variety of Python packages and I know that there are still some problems then I hope to get your feedback to improve it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cf3679a51c72dbe2d2549b5fe97e4ac5f1fa0fa0"
   },
   "source": [
    "you can follow me on:\n",
    "> ###### [ GitHub](https://github.com/mjbahmani)\n",
    "> ###### [LinkedIn](https://www.linkedin.com/in/bahmani/)\n",
    "> ###### [Kaggle](https://www.kaggle.com/mjbahmani/)\n",
    "\n",
    "--------------------------------------\n",
    "\n",
    " **I hope you find this kernel helpful and some upvotes would be very much appreciated**\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "<a id=\"55\"></a> <br>\n",
    "\n",
    "-----------\n",
    "\n",
    "# 9- References\n",
    "* [1] [Iris image](https://rpubs.com/wjholst/322258)\n",
    "* [2] [IRIS](https://archive.ics.uci.edu/ml/datasets/iris)\n",
    "* [3] [https://skymind.ai/wiki/machine-learning-workflow](https://skymind.ai/wiki/machine-learning-workflow)\n",
    "* [4] [IRIS-wiki](https://archive.ics.uci.edu/ml/datasets/iris)\n",
    "* [5] [Problem-define](https://machinelearningmastery.com/machine-learning-in-python-step-by-step/)\n",
    "* [6] [Sklearn](http://scikit-learn.org/)\n",
    "* [7] [machine-learning-in-python-step-by-step](https://machinelearningmastery.com/machine-learning-in-python-step-by-step/)\n",
    "* [8] [Data Cleaning](http://wp.sigmod.org/?p=2288)\n",
    "* [9] [competitive data science](https://www.coursera.org/learn/competitive-data-science/)\n",
    "\n",
    "\n",
    "-------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
